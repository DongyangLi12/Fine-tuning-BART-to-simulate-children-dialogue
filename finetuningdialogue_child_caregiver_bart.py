# -*- coding: utf-8 -*-
"""FineTuningDialogue_Child_Caregiver_BART

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B9ltp8kjA4IW2fynjsEy5Y_Uqn5v_a_x

# Training / Fine-tuning a Dialogue model based on a toddler

We are going to look at model fine-tuning by taking a general purpose language model and fine-tuning it to perform dialogue in the style of a toddler interacting with their caregiver
"""

!pip install accelerate -U
!pip install transformers -U
!pip install datasets
!pip install py7zr
!pip install tiktoken
!pip install sentencepiece
!pip install evaluate
!pip install rouge_score

import transformers
from transformers import pipeline, set_seed
import py7zr
import accelerate
import pandas as pd
import torch
import numpy as np

"""Download training and test data - selected and preprocessed pairs of caregiver utterances with child responses from this corpus:

https://childes.talkbank.org/access/Eng-UK/Thomas.html

https://childes.talkbank.org/access/Eng-UK/Manchester.html
"""

!gdown 1iY6xKKp455CCtoMBsONIQKtdarxchB-J

df = pd.read_csv("thomas-clean.csv")
df=df.dropna()
df_B = pd.read_csv("Becky.csv")
df_B=df_B.dropna()

from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from random import shuffle
ds=Dataset.from_pandas(df)
ds=ds.train_test_split(test_size=0.001,seed=99)
ds_B=Dataset.from_pandas(df_B)
ds_shuffled = ds_B.shuffle(seed=70)
ds_B = ds_shuffled.select(range(100))

from transformers import AutoTokenizer, BartForConditionalGeneration
device="cuda"
tokenizer = AutoTokenizer.from_pretrained("facebook/bart-base")
model = BartForConditionalGeneration.from_pretrained("facebook/bart-base").to(device)

"""Update the model vocabulary to include words from the child-caregiver speech corpus"""

text=ds["train"][:]["CONTEXT"]
text.extend(ds["test"][:]["CONTEXT"])
text.extend(ds["train"][:]["RESPONSE"])
text.extend(ds["test"][:]["RESPONSE"])

tokenset = list(set(str.split(' '.join(text))))
print(len(tokenizer))  # 28996
tokenizer.add_tokens(tokenset)
print(len(tokenizer))  # 28997

model.resize_token_embeddings(len(tokenizer))

"""Examine the performance of the untuned BART model in producing responses to the caregiver utterances in the test data"""

torch.cuda.empty_cache()
vanilla_predictions=[]
for i in range(ds["test"].shape[0]):
  input_ = tokenizer.batch_encode_plus(ds["test"][i:i+1]["CONTEXT"], max_length=1024, truncation=True, padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  responses = model.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         num_beams=100,
                         no_repeat_ngram_size=2,
                         early_stopping=True,
                         num_return_sequences=1,
                         max_length=1024)
  vanilla_predictions.extend(tokenizer.batch_decode(responses, skip_special_tokens=True))

vanilla_predictions

import evaluate
references=ds["test"][:]["RESPONSE"]
bleu = evaluate.load("bleu")
bleu.add(predictions=str(vanilla_predictions), references=str(references))
results = bleu.compute()
print(results)

import evaluate
references=ds["test"][:]["RESPONSE"]
rouge = evaluate.load("rouge")
rouge.add(predictions=str(vanilla_predictions), references=str(references))
results = rouge.compute()
print(results)

import math
def compute_perplexity(context, response):
    model.eval()


    input_ids = tokenizer(context, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)


    with tokenizer.as_target_tokenizer():
        labels = tokenizer(response, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss


    return math.exp(loss.item())

total_loss = 0
for context, response in zip(ds['test']['CONTEXT'], ds['test']['RESPONSE']):
    ppl = compute_perplexity(context, response)
    total_loss += math.log(ppl)

avg_loss = total_loss / len(ds['test']['CONTEXT'])
avg_perplexity = math.exp(avg_loss)

print(f"Average Perplexity: {avg_perplexity:.2f}")

"""### Fine-Tuning"""

def convert_examples_to_features(example_batch):
   input_encodings = tokenizer(example_batch["CONTEXT"], max_length=1024,
                               truncation=True)

   with tokenizer.as_target_tokenizer():
       target_encodings = tokenizer(example_batch["RESPONSE"], max_length=1024,
                                    truncation=True)

   return {"input_ids": input_encodings["input_ids"],
           "attention_mask": input_encodings["attention_mask"],
           "labels": target_encodings["input_ids"]}

dataset_pt = ds.map(convert_examples_to_features,
                                      batched=True)
columns = ["input_ids", "labels", "attention_mask"]
dataset_pt.set_format(type="torch", columns=columns)

from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer

seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(
   output_dir='dialogue-thomas', num_train_epochs=6, warmup_steps=500,
   per_device_train_batch_size=1, per_device_eval_batch_size=1,
   weight_decay=0.01, logging_steps=10, push_to_hub=False,
   eval_steps=250, save_steps=1e6,gradient_accumulation_steps=128)

trainer = Trainer(model=model, args=training_args,
                 tokenizer=tokenizer, data_collator=seq2seq_data_collator,
                 train_dataset=dataset_pt["train"],
                 eval_dataset=dataset_pt["test"])

!pip install wandb

import wandb
from huggingface_hub import notebook_login

notebook_login()
wandb.init(mode="disabled")

# hide_output
torch.cuda.empty_cache()
trainer.train()
# To save your fine-tuned model:
trainer.save_model("dialogue-thomas-model-bart")

"""T-BART-60e"""

# To use a model that has been pretrained for 60 epochs use the following lines
!gdown 1iZ2gFvbew-epRAMjkvlFwtAmPaQO4iJk
!gunzip dialogue-thomas-model-bart-60e.tar.gz
!tar xf dialogue-thomas-model-bart-60e.tar

from transformers import AutoTokenizer, BartForConditionalGeneration
model_ckpt="./dialogue-thomas-model-bart-60e"
#model_ckpt="./dialogue-thomas-model-bart-6e"
device="cuda"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model_tuned = BartForConditionalGeneration.from_pretrained(model_ckpt).to(device)

torch.cuda.empty_cache()
predictions=[]
for i in range(ds["test"].shape[0]):
  input_ = tokenizer.batch_encode_plus(ds["test"][i:i+1]["CONTEXT"], max_length=1024, truncation=True, padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  responses = model_tuned.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         num_beams=100,
                         no_repeat_ngram_size=2,
                         early_stopping=True,
                         num_return_sequences=1,
                         max_length=1024)
  predictions.extend(tokenizer.batch_decode(responses, skip_special_tokens=True))

predictions

import evaluate
references=ds["test"][:]["RESPONSE"]
bleu = evaluate.load("bleu")
bleu.add(predictions=str(predictions), references=str(references))
results = bleu.compute()
print(results)

import evaluate
references=ds["test"][:]["RESPONSE"]
rouge = evaluate.load("rouge")
rouge.add(predictions=str(predictions), references=str(references))
results = rouge.compute()
print(results)

import math
def compute_perplexity_tuned(context, response):
    model_tuned.eval()


    input_ids = tokenizer(context, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)


    with tokenizer.as_target_tokenizer():
        labels = tokenizer(response, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)

    with torch.no_grad():
        outputs = model_tuned(input_ids=input_ids, labels=labels)
        loss = outputs.loss


    return math.exp(loss.item())

total_loss = 0
for context, response in zip(ds['test']['CONTEXT'], ds['test']['RESPONSE']):
    ppl = compute_perplexity_tuned(context, response)
    total_loss += math.log(ppl)

avg_loss = total_loss / len(ds['test']['CONTEXT'])
avg_perplexity_tuned = math.exp(avg_loss)

print(f"Average Perplexity: {avg_perplexity_tuned:.2f}")

"""T-BART-6e"""

!gdown 1iYiJtoo1cM5v5oJgKZOlRCG-zM2yGKk5
!gunzip dialogue-thomas-model-bart-6e.tar.gz
!tar xf dialogue-thomas-model-bart-6e.tar

from transformers import AutoTokenizer, BartForConditionalGeneration
# model_ckpt="./dialogue-thomas-model-bart-60e"
model_ckpt_6e="./dialogue-thomas-model-bart-6e"
device="cuda"
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model_tuned_6e = BartForConditionalGeneration.from_pretrained(model_ckpt_6e).to(device)

torch.cuda.empty_cache()
predictions_6e=[]
for i in range(ds["test"].shape[0]):
  input_ = tokenizer.batch_encode_plus(ds["test"][i:i+1]["CONTEXT"], max_length=1024, truncation=True, padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  responses = model_tuned_6e.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         num_beams=100,
                         no_repeat_ngram_size=2,
                         early_stopping=True,
                         num_return_sequences=1,
                         max_length=1024)
  predictions_6e.extend(tokenizer.batch_decode(responses, skip_special_tokens=True))

predictions_6e

import evaluate
references=ds["test"][:]["RESPONSE"]
bleu = evaluate.load("bleu")
bleu.add(predictions=str(predictions_6e), references=str(references))
results = bleu.compute()
print(results)

import evaluate
references=ds["test"][:]["RESPONSE"]
rouge = evaluate.load("rouge")
rouge.add(predictions=str(predictions_6e), references=str(references))
results = rouge.compute()
print(results)

def compute_perplexity_tuned_6e(context, response):
    model_tuned_6e.eval()


    input_ids = tokenizer(context, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)


    with tokenizer.as_target_tokenizer():
        labels = tokenizer(response, return_tensors="pt", truncation=True, max_length=1024).input_ids.to(device)

    with torch.no_grad():
        outputs = model_tuned_6e(input_ids=input_ids, labels=labels)
        loss = outputs.loss


    return math.exp(loss.item())

total_loss = 0
for context, response in zip(ds['test']['CONTEXT'], ds['test']['RESPONSE']):
    ppl = compute_perplexity_tuned_6e(context, response)
    total_loss += math.log(ppl)

avg_loss = total_loss / len(ds['test']['CONTEXT'])
avg_perplexity_tuned_6e = math.exp(avg_loss)

print(f"Average Perplexity_6e: {avg_perplexity_tuned:.2f}")

"""Test the Becky Corpus (OOD testset) on T-BART-60e"""

torch.cuda.empty_cache()
predictions_B=[]
for i in range(ds_B.shape[0]):
  input_ = tokenizer.batch_encode_plus(ds_B[i:i+1]["CONTEXTS"], max_length=1024, truncation=True, padding='longest', return_tensors="pt")
  input_ids = input_['input_ids']
  input_mask = input_['attention_mask']
  responses = model_tuned.generate(input_ids=input_ids.to(device),
                         attention_mask=input_mask.to(device),
                         num_beams=100,
                         no_repeat_ngram_size=2,
                         early_stopping=True,
                         num_return_sequences=1,
                         max_length=1024)
  predictions_B.extend(tokenizer.batch_decode(responses, skip_special_tokens=True))

predictions_B

import evaluate
references=ds_B[:]["RESPONSES"]
bleu = evaluate.load("bleu")
bleu.add(predictions=str(predictions_B), references=str(references))
results = bleu.compute()
print(results)

import evaluate
references=ds_B[:]["RESPONSES"]
rouge = evaluate.load("rouge")
rouge.add(predictions=str(predictions_B), references=str(references))
results = rouge.compute()
print(results)

total_loss = 0
for context, response in zip(ds_B['CONTEXTS'], ds_B['RESPONSES']):
    ppl = compute_perplexity_tuned(context, response)
    total_loss += math.log(ppl)

avg_loss = total_loss / len(ds_B['CONTEXTS'])
avg_perplexity_tuned = math.exp(avg_loss)

print(f"Average Perplexity for Becky: {avg_perplexity_tuned:.2f}")